[{"id":0,"href":"/models-for-missing-data/docs/guide/analysis-inputs/","title":"Analysis inputs","section":"Guide","content":"Analysis inputs #  Analysis inputs consist of information about the sample design, the data themselves, and metadata used to specify models.\nSampling design #  Number of strata, stratum area, etc.\nMonitoring data #  Observations of various ecological indicator variables.\nModel metadata #  Models are specified using a metadata file that uses YAML syntax.\n"},{"id":1,"href":"/models-for-missing-data/docs/guide/usage/git/","title":"Getting started","section":"Usage","content":"Getting started #  Downloading the project for the first time #  To clone the entire repository (including submodules) run the following at the command line from the directory into which you\u0026rsquo;d like to place the project.\ngit clone --recurse-submodules --remote-submodules https://github.com/lzachmann/models-for-missing-data.git If the above doesn\u0026rsquo;t work, try updating Git (--remote-submodules is only available in newer versions of Git). Alternatively, try removing --remote-submodules from the clone command.\nSyncing an existing project to get the latest updates #  To get the latest updates for each of the submodules, run:\ngit submodule update --recursive --remote If you see error, likely you\u0026rsquo;ve made changes locally that you\u0026rsquo;ve not yet saved (staged and commited using git add and git commit). Git won\u0026rsquo;t replace changes in unsaved files with changes on the remote by default. This is desireable behavior. Try commiting your changes locally before syncing with the remote.\nPushing local changes to a subodule to its remote #  First, cd into the submodule directory, we\u0026rsquo;re going to do all of our Git work within the context of the submodule. If you\u0026rsquo;ve got uncommitted changes, do any necessary housekeeping:\ngit status git add . git commit -m \u0026#34;\u0026lt;some descriptive message about your changes\u0026gt;\u0026#34; As always, please ensure you\u0026rsquo;re not staging / committing unwanted files (e.g., binary files). Then run:\ngit fetch git checkout gh-submodule git merge \u0026lt;ref\u0026gt; git push origin gh-submodule "},{"id":2,"href":"/models-for-missing-data/docs/guide/","title":"Guide","section":"Docs","content":"Guide #  We describe analysis inputs, model parameterization options, and outputs.\nA sampling design ontology #  All networks tend to use different language to refer to the same basic observational and sampling units. In one network, the fundamental sampling unit, in which replicated observations are made, might be called SiteName, while in others it is Plot_ID or Plot or Loc_Name. As we shall see in the sampling design documentation, the name given by a network to these common samplign units does not matter, so long as it is defined in the network-level attributes file. The same reasoning applies to other features of the design, such as the name of columns used to indicate the park unit and stratum to which an observation belongs. Even datetime information is allowed to assume different formats, so long as the format of the date field is clearly specified.\nThe project file tree #  When downloading the repository containing all of the code, you will see several directories. Notably, the assets/, data-api/, and model-api/ directories, as well as the main calling script model-api/analysis-pipeline.R.\ngit clone --recurse-submodules git@github.com:lzachmann/models-for-missing-data.git You\u0026rsquo;ll see several additional files and directories that are oriented at documentation and reporting (docs/) and the maintanence of the software (docker/).\nFor Windows users #  Using Git Bash, cd into the directory into which you\u0026rsquo;d like to place the \u0026ldquo;models for missing data\u0026rdquo; tool. Then run:\ngit clone --recurse-submodules https://github.com/lzachmann/models-for-missing-data.git Protected assets #  "},{"id":3,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/model-block/likelihood/","title":"Likelihood","section":"Model Block","content":"Likelihood\nLikelihood (data model) #  likelihood: - binomial - beta-binomial  Options:\nDiscrete random variables\nCounts (see counts)\n poisson negative-binomial gen-pois zero-inflated-poisson zero-inflated-negative-binomial  The number of successes in a fixed number of trials Each trial has one of two possible outcomes: success (1) or failure (0). In this case we model the probability of success   \\(p\\)  on any given trial. Examples include point intercept data, etc.\n binomial  "},{"id":4,"href":"/models-for-missing-data/posts/non-ignorable-missingness/","title":"Non-ignorable missingness","section":"Posts","content":" Statistics is basically a missing data problem!\n \u0026ndash; Little 2013\nNearly all samples \u0026ndash; whether by design or by accident \u0026ndash; are incomplete. We very rarely make a complete census of all individuals in a population or all sites on a landscape. Sometimes we don\u0026rsquo;t collect, or can\u0026rsquo;t collect, complete information for individual samples or measures. For instance, we might know an animal was alive when it was last seen, so we know it survived at least that long, but know nothing about its current status. Or we might have information on the coverage of an invasive species down to a certain patch size, beyond which patches are too small or numerous to survey.\nAll of these examples involve missing data. It probably goes without saying that estimating population level parameters in light of such \u0026ldquo;missingness\u0026rdquo; is a challenge. We cannot compute a population mean if one or more values are missing.\n 6 NA 9 NA 9 NA 5 6 9 7 2  The mean for such a string of numbers is undefined.1 In some settings these missing data can be safely ignored, but these circumstances are rarely met in ecological studies. One helpful \u0026ldquo;trick\u0026rdquo; is to consider how the problem would be solved had the data been complete, and to consider what we might do to arrive at complete data. An incomplete data perspective provides a general framework for resolving this sampling problem and making inference.\n  If you\u0026rsquo;re an R person and you thought, \u0026ldquo;why not just include an na.rm = TRUE in the call to mean()\u0026rdquo;, kudos! You would indeed obtain a real number, but we don\u0026rsquo;t yet whether we can safely ignore these NA values.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":5,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/data-block/response-info/","title":"Response info","section":"Data Block","content":"Response information #  Here is the first few lines of the data block for a model of count-type data at Rocky Mountain Network called rich.yml, which lives in the directory config/ROMN/LIBI\n# ==== DATA =================================================================== response info: file: data/ROMN/LIBI_richness_20171208_r.csv state variable: response column: - native.rich - native.forb.rich description: - Native species richness - Native forb species richness sampling method: plot sample id column(s): - Transect - Plot "},{"id":6,"href":"/models-for-missing-data/docs/guide/analysis-inputs/sampling-design/","title":"Sampling design","section":"Analysis inputs","content":"Sampling design #  Two high-level \u0026ldquo;attributes\u0026rdquo; files are required for all analyses. So long as the basic details of the sampling design they describe do not vary for different measures within a park, or different park units within a network, they only need to be specified once.\nThe config directory #  The file tree needed to configure analysis of data from a given park unit (\u0026lt;unit code\u0026gt;) in a given network (\u0026lt;network code\u0026gt;) looks like the following\n. # project root └── config/ └── \u0026lt;network code\u0026gt;/ ├── \u0026lt;unit code\u0026gt;/ │ ├── _park-level-attributes.yml │ └── \u0026lt;analysis file\u0026gt;.yml └── _network-level-attributes.yml  The directory structure, as shown, is relative to the root of the project on your file system. The contents of \u0026lt;analysis file\u0026gt;.yml are described in the model metadata section. To take a real-world example from an analysis of richness data at Little Bighorn Battlefield National Monument (LIBI) Rocky Mountain Network (ROMN), this might look like\n. └── config/ └── ROMN/ ├── LIBI/ │ ├── _park-level-attributes.yml │ └── richness.yml └── _network-level-attributes.yml  Network-level attributes #  These attributes, which appear in the file _network-level-attributes.yml, consist of several key-value pairs used to define the park unit code (unit code column) by whatever name it goes by in the raw data, the name of the column containing the site ID (site id column), and a nested set of key-value pairs used to describe the date associated with each observation.\nunit code column: Park site id column: SiteName event date info: date-time column: Year date-time format: Y! # see lubridate::parse_date_time() for details You can also specify multiple date-time formatting options if individual data files for a network use different datetime standards.\nunit code column: Unit_Code site id column: Plot_ID event date info: date-time column: Start_Date date-time format: - Y!-m!*-d! I!:M!:S!  # see lubridate::parse_date_time() for details - Y!-m!*-d! Park-level attributes #  Park-level metadata, stored in _park-level-attributes.yml, cannot be defined at a higher level because most, if not all, of this information applies only to an individual park, and not to other park units in the network. Specifically, we use this file to describe the column in the raw data that defines the strata (stratum id column), if present and the associated stratum areas (stratum area info). These areas, in turn, are used internally to computed a weighted mean at the park scale. Each entry beneath stratum area info must correspond to the actual stratum IDs in the column indicated by stratum id column. If there are no strata, then you can supply the same column used by _network-level-attributes.yml to indicate the unit code, and supply a single stratum area for the park.\nstratum id column: MDCATY stratum area info: Gulley1: 684731 Gulley2: 138247 Upland: 1063552 The number assigned to each stratum under stratum area info corresponds to the total number of sites \u0026ndash; regardless of the actual units \u0026ndash; that could have been sampled in that stratum. In this example there are 684731 in Gulley1 Thus, total number of sites sampled is a negligible proportion of the landscape. 684731 / (684731 + 138247 + 1063552) = 0.3629579. Does the unit of the stratum areas matter so long as they are in the same unit? No, not yet, because these are only used to create the (normalized) stratum weights.\n"},{"id":7,"href":"/models-for-missing-data/posts/trend-vs-trajectory/","title":"Disentangling concepts of status, trend, and trajectory","section":"Posts","content":"The terms status and trend are ubiquitous in resource monitoring and management settings. To be useful and robust, however, they require precise (mathematical) definitions. It has been my experience that misunderstanding these terms can lead to misapplication of model predictions and to researchers and managers drawing the wrong conclusions from the data. In this post we show how relatively simple, even intuitive, definitions for each of these terms clarifies their intent, and improves the insights provided by models of monitoring data.\nFor context, it is helpful to see how these terms have been used previously in the literature\n The ‘term “trend” [is] a description of an overall tendency, without regard to fluctuations in the trajectory. The distinction between the terms has important consequences for analysis and interpretation of survey data. Trend describes the change in a population over a specific interval; trajectory describes the manner in which the change occurred.\n \u0026ndash; Link and Sauer 1998\nTrend vs. trajectory (Citation: Link\u0026#32;\u0026amp;\u0026#32;Sauer,\u0026#32;1998Link,\u0026#32; W.\u0026#32;\u0026amp;\u0026#32;Sauer,\u0026#32; J. \u0026#32; (1998). \u0026#32;Estimating population change from count data: Application to the north american breeding bird survey. Ecological applications,\u0026#32;8(2).\u0026#32;258–268. ) .\n References #    Link\u0026#32;\u0026amp;\u0026#32;Sauer (1998)  Link,\u0026#32; W.\u0026#32;\u0026amp;\u0026#32;Sauer,\u0026#32; J. \u0026#32; (1998). \u0026#32;Estimating population change from count data: Application to the north american breeding bird survey. Ecological applications,\u0026#32;8(2).\u0026#32;258–268.     "},{"id":8,"href":"/models-for-missing-data/docs/faq/","title":"FAQ","section":"Docs","content":"Frequently asked questions #  Q: Which type of data am I working with?\nA: Let\u0026rsquo;s talk about it!\nQ: Do we use stratum weights when we do the finite population correction? A: Yes.\nQ: Should the locations I supply be for sites or samples within sites? A: For now, just the site centroids.\nQ: Why do my variography plots show only a fraction of the largest distance between sites? A: The R functions we are using to develop these plots only show semivariance for distances up to 1/3 of the maximum observed distance.\n"},{"id":9,"href":"/models-for-missing-data/docs/guide/usage/gui/","title":"Graphical user interface","section":"Usage","content":"Graphical user interface (RStudio) #  "},{"id":10,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/model-block/link/","title":"Link function","section":"Model Block","content":"Deterministic (link) function for the regression on the mean #  deterministic model: - inverse-logit  "},{"id":11,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/","title":"Metadata syntax","section":"Analysis inputs","content":"Metadata syntax for specifying models #  Fitting models requires a metadata file that uses YAML syntax.\nYAML syntax #  Among a few other arguments, most of which have defaults, the analysis pipeline requires a metadata file. While the filename is arbitrary, the path to the file is not. The information in this file defines inputs, the model(s) being considered, and other actions.\n"},{"id":12,"href":"/models-for-missing-data/docs/guide/analysis-inputs/monitoring-data/","title":"Monitoring data","section":"Analysis inputs","content":"Monitoring data #  The data exists in a file structure that is similar to that seen in the config directory, but is generally more flexible beneath the network-level of the hierarchy. Data can be hosted in comprehensive files containing all observations from all park units in a given network. Alternatively, data can be managed more modularly at a park scale.\nThe data directory #  "},{"id":13,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/data-block/site-loc-info/","title":"Site location","section":"Data Block","content":"Site location information [optional] #  "},{"id":14,"href":"/models-for-missing-data/docs/guide/usage/cli/","title":"Command line interface","section":"Usage","content":"Command line interface #  Run an analysis utilizing 3 CPUs with, for example\n# ./analysis-pipeline.r \\ -f ./config/ROMN/GRKO/veg-categories-rel-cover.yml \\ -a 5000 -u 50000 -n 15000 \\ -c 3 The (optional) -a, -u, -n, and -p flags control the number of iterations for adaptation, \u0026lsquo;burnin\u0026rsquo;, and sampling, and whether or not the pipeline is executed in parallel, respectively. The defaults are 3K, 15K, and 5K iterations, and FALSE, respectively.\nAll outputs (including logs) are stored in the \u0026lsquo;output\u0026rsquo; directory, which mirrors \u0026ndash; almost exactly \u0026ndash; the structure of the \u0026lsquo;config\u0026rsquo; directory. The only difference is that the results of analyses for each state variable are stored separately, whereas the configuration of analyses for like-variables (related count or cover variables, for example) can be specified simultaneously in the same YAML file.\n"},{"id":15,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/data-block/covariate-info/","title":"Covariates","section":"Data Block","content":"Covariates #  "},{"id":16,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/model-block/group-level-effects/","title":"Group-level effects","section":"Model Block","content":"Group-level (random) effects #  # Group-level effects structure. One of: b0, b0-b1. group-level effects: - b0 - b0-b1  "},{"id":17,"href":"/models-for-missing-data/posts/unequal-inclusion-probability/","title":"Unequal inclusion probabilities","section":"Posts","content":"The Sonoran Desert is among the most extreme environments on Earth. Sampling in these remote, rugged landscapes requires a different approach. When the Park Service established monitoring in Organ Pipe Cactus National Monument they used an approach to select sites based on the cost of travel to sites on the broader landscape, visiting less \u0026ldquo;costly\u0026rdquo; sites with higher probability than more costly sites. The cost surface that defined the probability of inclusion of sites was developed using terrain data, and a tool that estimates the time to travel to any arbitrary location on the landscape.\n"},{"id":18,"href":"/models-for-missing-data/docs/guide/usage/model-builder/","title":"Model builder","section":"Usage","content":"Model builder #  The shell script compile-jags-file.sh builds JAGS model files using several arguments. The arguments \u0026ndash; which correspond to the likelihood, the deterministic function, group-level effects parameterization (random intercepts and slopes vs. random intercepts only), the presence or absence of additional covariates and, finally, the path to write to \u0026ndash; are supplied via the \u0026lsquo;MODEL\u0026rsquo; block in the YAML file for an analysis. For purposes of development or debugging, compile-jags-file.sh can be sourced from the command line with an optional output directory as such:\n./src/model-builder/compile-jags-file.sh \\ \u0026lt;likelihood\u0026gt; \u0026lt;deterministic-function\u0026gt; \u0026lt;group-level-effects\u0026gt; \u0026lt;covariates\u0026gt; \\ \u0026lt;write-path\u0026gt; For example:\n./src/model-builder/compile-jags-file.sh \\ poisson exponential b0-b1 w1 \\ assets The model compiler supports both random intercepts (b0) or random intercepts / random slopes (b0-b1) models with and without covariates.\n"},{"id":19,"href":"/models-for-missing-data/posts/sampling-and-populations/","title":"Sampling and populations","section":"Posts","content":"We sample for a very practical reason. It\u0026rsquo;s usually impossible to get information on the whole population, so we use a sample to make inferences about the population. In our case, the population is typically all sites in a stratum or all sites \u0026ndash; in all strata \u0026ndash; at the scale of an entire park. Typically, the inference we seek entails three questions.\n What\u0026rsquo;s the best estimate of the population mean?   We can generate a sample mean,   \\(\\bar{x}\\)  , from our sample. This is the best estimate of the population mean.\n  \\[\\bar{x}=\\frac{\\sum{x}}{n}\\]  How confident are we about that estimate?   Because we have a small sample, we can\u0026rsquo;t be sure the sample mean is exactly the population mean. There\u0026rsquo;s uncertainty around that estimate. The standard error of the sample mean,  \\(\\text{SE}(\\bar{x})\\)  , relates to how uncertain we are about our estimate. The higher it is, the more uncertain we are that our sample mean reflects the true population mean. Small samples generally yield larger standard errors.\n  \\[\\text{SE}(\\bar{x})=\\frac{s}{\\sqrt{n}}\\]  What is our best estimate of the population standard deviation?1   The sample standard deviation,  \\(s\\)  , can give us information about the possible population standard deviation \u0026ndash; how spread the population is likely to be with respect to the measure of interest.\n  \\[s=\\sqrt{\\frac{\\sum{(x - \\bar{x})^2}}{n-1}}\\]  These three measures form the basic substrate of our understanding of the populations we are sampling, and are all essential to decision making.\nBelow, we consider three scenarios, sampling from an infinite population, a finite population, and the full population. The goal of the examples below is to show what sampling is all about. As we shall see, the finite population case is the bridge between the other two edge cases \u0026ndash; the infinite and full population instances.\nAn infinite population #  This is a very typical scenario. Of course, I\u0026rsquo;m not aware of any truly infinite populations. What we mean, instead, is that our sample of the population,  \\(n\\)  , is small enough that\u0026hellip; blah blah blah. As such, we can think of the population as at least theoretically infinite.\nExample Estimate the average number of trees in sites #  Given a sample of  (n=5)  sites with observations\u0026hellip;, the sample mean is XX. This our best estimate of the true population mean. But what is the population we are referring to? Well, it\u0026rsquo;s all possible sites. Of course, there\u0026rsquo;s not an infinite number of sites, but it\u0026rsquo;s large enough that we can treat it as though it is infinite. How confident are we in this estimate? If we took a different sample, the sample mean would be different, but how different? This is where the standard error of the sample mean comes in\u0026hellip;   A finite population2 #  Not particularly common, but it\u0026rsquo;s useful to look at because it ties together both the infinite population situation and the final example.\n \\[\\text{SE}(\\bar{x})=\\frac{s}{\\sqrt{n}}\\sqrt{\\frac{N - n}{N - 1}}\\]  The standard error of the mean in this case will be less than in the infinite population case, because we have proportionately more information about the population that we do in the first example.\nThe full population #  Technically no longer a sample. We have the full population in hand. There\u0026rsquo;s no observations that site outside of the sample that we care about.\n \\[\\mu=\\frac{\\sum{x}}{N}\\]  How confident are we in this estimate? Perfectly confident:  \\(\\text{SE}(\\mu)=0\\)  ! Why? We can still think of this as a sample from a finite population, but the number of observations in the sample equals the number of observations in the population. The formula for the standard error of the mean for a finite population still applies. The numerator in the second term  \\(N-n\\)  becomes zero, and the whole thing collapses to zero.3\nWe can find the population standard deviation.\n \\[\\sigma=\\sqrt{\\frac{\\sum{(x - \\mu)^2}}{N}}\\]    Note that part of the  \\(\\text{SE}(\\bar{x})\\)  is in fact  \\(s\\)  .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Sometimes called \u0026ldquo;sampling without replacement\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Note that the same basic reasoning can be applied to the first example, as well. If we were to look at an infinite population and sub in Inf for the capital Ns. The second term resolves to 1. Inf on inf is going to be 1, the sqrt of which is also 1, so the second term dissapears and you\u0026rsquo;re left with the original formula for the standard error.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":20,"href":"/models-for-missing-data/docs/guide/usage/","title":"Usage","section":"Guide","content":"Usage #  Running the analysis pipeline can be done in several ways.\n"},{"id":21,"href":"/models-for-missing-data/posts/interpreting-coefficients/","title":"Interpreting coefficients","section":"Posts","content":"Making sense of the effects of variables included as predictors #  Some aspects of covariate effects are readily apparent \u0026ndash; for instance, the sign of a coefficient in a model says at least something about the general directionality of the effect, positive or negative. However, a deeper understanding of a model typically requires inferences that go well beyond simple measures of the directionality or significance of effects \u0026ndash; it requires understanding the size of effects.\nWhat do we mean by effect size, and how is it measured? In order to offer a working definition of effect size, we need to use a bit of math. The size of an effect can be approximated as the average change in the response variable being modeled, per unit change in some predictor variable, which we\u0026rsquo;ll call   \\(x_1\\)   \\[\\mu_i = g(\\beta_0 \u0026#43; \\beta_1 x_{1i}).\\]  The parameter  \\(\\beta_0\\)  is the intercept and  \\(\\beta_1\\)  is the slope associated with covariate  \\(x_1\\)  in a generalized linear model (linear, exponential, or inverse logit), which we represent using the link function  \\(g()\\)  . The effect of  \\(x_1\\)  translates into changes in the mean of the response variable through  \\(\\beta_1\\)  and link function  \\(g()\\)  . To understand the effect that  \\(x_1\\)  will have on the mean on the scale of the actual data, we first have to take note of the function for which  \\(g()\\)  is a placeholder, as well as any transformations (e.g., scaling) that may have been applied to the data.\nExample | the response of plant cover to rainfall #  Let\u0026rsquo;s consider a covariate that we might expect to have a mechanistic influence on plant cover, such as accumulated spring precipitation. Thus,  \\(x_1\\)  is a measure of rainfall. The plant cover observations were collected at point intercepts along a transects at a site, indexed by subscript  \\(i\\)  . At any given point intercept, plants are either present or absent.  \\(y_i\\)  , then, is the sum of the number of times plants were detected (the total number of \u0026ldquo;hits\u0026rdquo;) at any point intercept along a transect. The observations are modeled as\n \\[\\begin{align*} p_i \u0026amp;= \\text{logit}^{-1}(\\beta_0 \u0026#43; \\beta_1 x_{1i}) \\\\ y_i \u0026amp;\\sim \\text{binomial}(n, p_i) \\end{align*}\\]  where  \\(n\\)  is the total number of point intercepts evaluated along each transect (the number of \u0026ldquo;trials\u0026rdquo;).  \\(x_1\\)  is scaled from its original units (cm) to zero mean and unit variance prior to fitting the model.\nLet\u0026rsquo;s also take as a given that the estimated median value of the coefficients is as follows:  \\(\\beta_0=0.12\\)  and  \\(\\beta_1=0.22\\)  . Prior to scaling,  \\(x_1\\)  had mean 20.4, standard deviation 15.3, and range [0.4, 70.9]. To better understand the influence of  \\(x_1\\)  on the mean plant cover, we will create a new vector called  \\(x_1^{\\text{pred}}\\)  , which consists of scaled rainfall values at equally spaced intervals over the range of  \\(x_1\\)  . Here is the basic setup, in  \\(\\textsf{R}\\)  :\nb0 \u0026lt;- 0.12 # the intercept b1 \u0026lt;- 0.22 # the effect of spring precipitation x1_mean \u0026lt;- 20.4; x1_sd \u0026lt;- 15.3; x1_range \u0026lt;- c(0.4, 70.9) x1_pred_raw \u0026lt;- seq(x1_range[1], x1_range[2], length.out = 100) x1_pred \u0026lt;- (x1_pred_raw - x1_mean) / x1_sd Let\u0026rsquo;s plot mean plant cover, p, over the range of x1_pred:\np \u0026lt;- boot::inv.logit(b0 + b1 * x1_pred) p_x1_mean \u0026lt;- boot::inv.logit(b0) # p at average precip plot(x1_pred_raw, p, ylim = c(0, 1), type = \u0026#39;l\u0026#39;, col = \u0026#39;red\u0026#39;) abline(v = x1_mean, lty = 2, col = \u0026#39;gray\u0026#39;) abline(h = boot::inv.logit(b0), lty = 2, col = \u0026#39;gray\u0026#39;) Mean plant cover increases as a function of spring rainfall, from a value of approximately 0.46 at its minimum (next to zero precipitation) to 0.70 at its maximum (~71 cm of rainfall). We can interpret the quantity  \\(\\text{logit}^{-1}(\\beta_0)=\\text{logit}^{-1}(0.12)=0.53\\)  (the horizontal dashed gray line) as the mean site-level cover of plants in a year with average spring rainfall (vertical dashed gray line).1\n This being an inverse logit model, we are somewhat obligated to talk about odds, which are given by the quantity  \\(\\frac{p}{(1-p)}\\)  . In this example, it\u0026rsquo;s the ratio of the probability of a plant being present at a given point intercept to the probability of a point intercept not touching a plant. In a year with average spring rainfall, you are ~1.13X more likely to see it than not (0.53 / (1 - 0.53)).2\n \\(\\beta_1\\)  is the multiplicative change in odds per standard deviation spring precipitation.3 To compute the odds of seeing plant cover if spring rainfall is one standard deviation higher than average we would use\n(odds_x1_mean \u0026lt;- p_x1_mean / (1 - p_x1_mean)) exp(b1) * odds_x1_mean # 1.4 At 1.4X, the odds of encountering plants has gone up. Basically, when rainfall is one standard deviation over the mean, you\u0026rsquo;re now 1.4X more likely to see it than not. Thus, the odds of seeing plants increases with increasing precipitation. We can show the same thing using slightly different math.\np_x1_sd1 \u0026lt;- boot::inv.logit(b0 + b1 * 1) # let\u0026#39;s make sure we can recover the same odds using different math p_x1_sd1 / (1 - p_x1_sd1) # again, 1.4! Let\u0026rsquo;s show the odds at progressively more extreme values of rainfall ( \\(1\\sigma, 2\\sigma, 3\\sigma\\)  ):\nplot(x1_pred_raw, p, ylim = c(0, 1), type = \u0026#39;l\u0026#39;, col = \u0026#39;red\u0026#39;) abline(v = x1_mean, lty = 2, col = \u0026#39;gray\u0026#39;) abline(h = p_x1_mean, lty = 2, col = \u0026#39;gray\u0026#39;) # at 1 sd lines(x = rep(x1_mean + x1_sd, 2), y = c(0, boot::inv.logit(b0 + b1 * 1)), lty = 2, col = \u0026#39;orange\u0026#39;) points(x = x1_mean + x1_sd, y = boot::inv.logit(b0 + b1 * 1)) # at 2 sd lines(x = rep(x1_mean + 2*x1_sd, 2), y = c(0, boot::inv.logit(b0 + b1 * 2)), lty = 2, col = \u0026#39;green\u0026#39;) points(x = x1_mean + 2*x1_sd, y = boot::inv.logit(b0 + b1 * 2)) # at 3 sd lines(x = rep(x1_mean + 3*x1_sd, 2), y = c(0, boot::inv.logit(b0 + b1 * 3)), lty = 2, col = \u0026#39;blue\u0026#39;) points(x = x1_mean + 3*x1_sd, y = boot::inv.logit(b0 + b1 * 3)) When rainfall is near it\u0026rsquo;s observed maximum value (at 3 standard deviations higher than the mean), you are now exp(b1)^3 or ~2.2X more likely to see plants than not.\n Effect size inference using another scale #  Let\u0026rsquo;s say the manager isn\u0026rsquo;t thrilled about using standard deviations in rainfall as the unit of measure. Instead, they\u0026rsquo;re focused on inches of precipitation. How do we cast these effect size calculations in terms of inches and not standard deviations? First we need to recover the unstandardized slope term, which we\u0026rsquo;ll notate as  \\(\\beta_1^{\\prime}\\)  using  \\(\\beta_1^{\\prime}=\\frac{\\beta_{1}}{\\text{sd}(\\mathbf{x}_{1})}\\)  .\nb1_unscaled \u0026lt;- b1 / x1_sd exp(b1_unscaled) # per cm exp(b1_unscaled * 2.54) # per inch exp(b1_unscaled) is the multiplicative change in odds per cm spring rainfall. It\u0026rsquo;s much smaller than exp(b1) because x1_sd is ~15cm of precip!). We now want inches. So it\u0026rsquo;s b1_unscaled * 2.54.\n  After scaling, the value of  \\(x_1\\)  in an average rainfall year is simply zero. The term involving rainfall in the model drops out because  \\(\\beta_1 \\times 0 = 0\\)  and  \\(\\text{logit}^{-1}(0.12 \u0026#43; 0)\\)  simplifies to  \\(\\text{logit}^{-1}(0.12)\\)  .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The odds here make at least some intuitive sense. At p = 0.53 you\u0026rsquo;re just slightly more likely than not to encounter plants at a given point at this site.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Why is this the case? Inverse logit models have the form  \\[\\begin{align*} p_{i} \u0026amp;= \\frac{e^{\\beta_{0}\u0026#43;\\beta_1x_{1i}}}{1\u0026#43;e^{\\beta_{0}\u0026#43;\\beta_1x_{1i}}} \\\\ \u0026amp;= \\text{logit}^{-1}(\\beta_0 \u0026#43; \\beta_1 x_{1i}) \\end{align*}\\]  Rearranging the equation above, we obtain  \\[\\text{logit}(p_{i})=\\text{log}\\left(\\frac{p_{i}}{1-p_{i}}\\right)=\\beta_{0}\u0026#43;\\beta_1x_{1i}\\]   \\(\\beta_1\\)  can be understood somewhat more intuitively by exponentiating both sides of the equation above  \\[\\begin{align*} \\frac{p_{i}}{1-p_{i}} \u0026amp;= e^{\\beta_{0}\u0026#43;\\beta_1x_{1i}}\\\\ \u0026amp;= e^{\\beta_{0}} \\times e^{\\beta_1x_{1i}} \\end{align*}\\]  \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":22,"href":"/models-for-missing-data/posts/stratum-varying-fixed-effects/","title":"Stratum-varying fixed effects","section":"Posts","content":"Assume we have three strata,   \\(s_0\\)  ,  \\(s_1\\)  , and  \\(s_2\\)  , where  \\(s_0\\)  is the \u0026ldquo;reference\u0026rdquo; stratum – in other words,  \\(s_0\\)  is the stratum for which the 0/1 indicator is 0 across the board in the indicator matrix below (the first row):\n \\[\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}\\]  B_0 + (B_1 + B_1_s1_offset * s1 + B_1_s2_offset * s2) * x_1 # in stratum s0 B_0 + (B_1) * x_1 # in stratum s1 B_0 + (B_1 + B_1_s1_offset * s1) * x_1 # in stratum s2 B_0 + (B_1 + B_1_s2_offset * s2) * x_1 # lm(y~x1*x2) model.matrix(~x1*x2, tibble(x1 = runif(5), x2 = runif(5))) "},{"id":23,"href":"/models-for-missing-data/posts/offsets/","title":"The offset term","section":"Posts","content":"Counts of things naturally scale with the length or duration of observation, the area sampled, and sampling intensity (Citation: McElreath,\u0026#32;2018McElreath,\u0026#32; R.\u0026#32; (2018). \u0026#32;Statistical rethinking: A bayesian course with examples in r and stan. \u0026#32; Chapman; Hall/CRC. ) . For instance, the longer the river stretch we survey, the more fish we\u0026rsquo;ll tend to find.\nOffset terms are used to model rates \u0026ndash; e.g., counts per unit area or time. In the context of the model, the offset term transforms the response variable from a rate to a count.\nWhy do we need an offset? #  Why can\u0026rsquo;t we simply derive a new response variable by normalizing the original counts by the relevant sampling unit? The distributions we use to model counts have support for discrete-valued variables. Transformations of the counts can violate that requirement. For instance, say we counts signs of human disturbance in 20m-by-50m plots. If we see 60 signs, that\u0026rsquo;s 60 / (20 * 50) = 0.06. Although the mean of a Poisson distributed variable can be continuous, observations cannot be. Thus, transforming the response variable from counts to a rate is incorrect.\nThe offset needs to be on the same scale as the linear predictor. In the case of a log link model, this requires the offset variable to be logged before inclusion in the model  poi_r \u0026lt;- glm(numclaims ~ x1+x2+x3,data=train, family = \u0026ldquo;poisson\u0026rdquo;, offset=log(exposure))\nTypically we use offsets when the sampling unit (time, area, exposure) varies across observations. For example, species richness in riparian areas in arid ecosystems is often evaluated for the entire riparian area, which tend to be quite small. But the actual extent of each riparian area might differ.\n References #    McElreath (2018)  McElreath,\u0026#32; R.\u0026#32; (2018). \u0026#32;Statistical rethinking: A bayesian course with examples in r and stan. \u0026#32; Chapman; Hall/CRC.     "},{"id":24,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/data-block/","title":"Data Block","section":"Metadata syntax","content":"Data specification #  "},{"id":25,"href":"/models-for-missing-data/docs/guide/analysis-inputs/metadata-syntax/model-block/","title":"Model Block","section":"Metadata syntax","content":"Model specification information #  "},{"id":26,"href":"/models-for-missing-data/docs/guide/hidden/","title":"Hidden","section":"Guide","content":"This page is hidden in menu #  Quondam non pater est dignior ille Eurotas #  Latent te facies #  Lorem markdownum arma ignoscas vocavit quoque ille texit mandata mentis ultimus, frementes, qui in vel. Hippotades Peleus pennas conscia cuiquam Caeneus quas.\n Pater demittere evincitque reddunt Maxime adhuc pressit huc Danaas quid freta Soror ego Luctus linguam saxa ultroque prior Tatiumque inquit Saepe liquitur subita superata dederat Anius sudor  Cum honorum Latona #  O fallor in sustinui iussorum equidem. Nymphae operi oris alii fronde parens dumque, in auro ait mox ingenti proxima iamdudum maius?\nreality(burnDocking(apache_nanometer), pad.property_data_programming.sectorBrowserPpga(dataMask, 37, recycleRup)); intellectualVaporwareUser += -5 * 4; traceroute_key_upnp /= lag_optical(android.smb(thyristorTftp)); surge_host_golden = mca_compact_device(dual_dpi_opengl, 33, commerce_add_ppc); if (lun_ipv) { verticalExtranet(1, thumbnail_ttl, 3); bar_graphics_jpeg(chipset - sector_xmp_beta); }  Fronde cetera dextrae sequens pennis voce muneris #  Acta cretus diem restet utque; move integer, oscula non inspirat, noctisque scelus! Nantemque in suas vobis quamvis, et labori!\nvar runtimeDiskCompiler = home - array_ad_software; if (internic \u0026gt; disk) { emoticonLockCron += 37 + bps - 4; wan_ansi_honeypot.cardGigaflops = artificialStorageCgi; simplex -= downloadAccess; } var volumeHardeningAndroid = pixel + tftp + onProcessorUnmount; sector(memory(firewire + interlaced, wired)); "},{"id":27,"href":"/models-for-missing-data/docs/guide/model-info/data-types/","title":"Data Types","section":"Model info","content":"Data types #     Type of data Example Probability distribution(s) Deterministic model(s)     Counts The number of native species on a 1 m^2 quadrat poisson, negative-binomial, or gen-pois1 exponential, linear, or monomolecular   Continuous and non-negative Basal gap sizes along a transect lognormal or gamma exponential or linear   Presence-absence (zero or one) The occurrence (or not) of signs of human disturbance in a plot bernoulli inverse-logit   Counts in two categories The number of point intercepts (n = 100) at which non-native forbs are encountered along a transect binomial or beta-binomial inverse-logit   Proportion A visual estimate of bare ground cover on a 1 m^2 quadrat beta inverse-logit   Ordinal The cover class of forbs hurdle-ordinal-latent-beta inverse-logit      Or their zero-inflated counterparts \u0026ndash; zero-inflated-poisson or zero-inflated-negative-binomial\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":28,"href":"/models-for-missing-data/docs/guide/model-info/data-types/counts/","title":"Counts","section":"Data Types","content":"Counts #  Counts of things are modeled using the Poisson or negative binomial distributions. The latter is typically required with the data are overdispersed. Underdispersed data can be modeled using the generalized Poisson distribution. We also support zero-inflated version of each.\nPoisson #  Discrete random variables that occur randomly over time or space.\nMath Likelihood #    \\[y \\sim \\text{Poisson}(\\lambda)\\]  where  (\\lambda)  is the mean.Code JAGS code #  y[n] ~ dpois(mu[n])   "},{"id":29,"href":"/models-for-missing-data/docs/guide/model-info/data-types/proportions/","title":"Proportions","section":"Data Types","content":"Proportions #  Quantities that can be expressed as a proportion. Plant cover observations made by ocular estimation, etc.\nBeta #  Continuous random variables that assume values between 0 and 1.   \\[y \\sim \\text{beta}(\\alpha, \\beta)\\]  where \u0026lt;insert big moment match?\u0026gt;\n"}]
[{"id":0,"href":"/models-for-missing-data/docs/guide/config-files/design-metadata/","title":"Design metadata","section":"Config files","content":"Design metadata #  Two high-level \u0026ldquo;attributes\u0026rdquo; files are required for all analyses. So long as the basic details of the sampling design they describe do not vary for different measures within a park, or different park units within a network, they only need to be specified once.\nThe config file tree #  The file tree needed to configure analysis of data from a given park unit (\u0026lt;unit code\u0026gt;) in a given network (\u0026lt;network code\u0026gt;) looks like the following\n. # project root └── assets └── \u0026lt;config dir\u0026gt;/ └── \u0026lt;network code\u0026gt;/ ├── \u0026lt;unit code\u0026gt;/ │ ├── _park-level-attributes.yml │ └── \u0026lt;analysis file\u0026gt;.yml └── _network-level-attributes.yml  The name of \u0026lt;config dir\u0026gt; is arbitrary. For the demo analyses it\u0026rsquo;s _config, while for the \u0026ldquo;uplands\u0026rdquo; status and trends work we\u0026rsquo;ve used uplands-config.\nThe directory structure, as shown, is relative to the root of the project on your file system. The contents of \u0026lt;analysis file\u0026gt;.yml are described in the ensuing sections of the guide. To take a real-world example from an analysis of species richness data at Little Bighorn Battlefield National Monument (LIBI), in Montana \u0026ndash; a park within Rocky Mountain Network (ROMN) \u0026ndash; the complete file tree might look like the following:\n. └── assets └── uplands-config/ └── ROMN/ ├── LIBI/ │ ├── _park-level-attributes.yml │ └── richness.yml └── _network-level-attributes.yml  Network-level attributes #  These attributes, which appear in the file _network-level-attributes.yml, consist of several key-value pairs used to define the park unit code (unit code column) by whatever name it goes by in the raw data, the name of the column containing the site ID (site id column), and a nested set of key-value pairs used to describe the date associated with each observation.\nunit code column: Park site id column: SiteName event date info:  date-time column: Year  date-time format: Y! # see lubridate::parse_date_time() for details You can also specify multiple date-time formatting options if individual data files for a network use different datetime standards. Here\u0026rsquo;s another example of a _network-level-attributes.yml file for analyses from Northern Colorado Plateau Network:\nunit code column: Unit_Code site id column: Plot_ID event date info:  date-time column: Start_Date  date-time format:  - Y!-m!*-d! I!:M!:S!  # see lubridate::parse_date_time() for details  - Y!-m!*-d! Park-level attributes #  Park-level metadata, stored in _park-level-attributes.yml, cannot be defined at a higher level because most, if not all, of this information applies only to an individual park, and not to other park units in the network. Specifically, we use this file to describe the name of the column in the raw data that defines the strata (stratum id column) and the associated stratum areas (stratum area info).\nThese areas, in turn, are used internally to compute a weighted mean at the park scale. Each entry beneath stratum area info must correspond to the actual stratum IDs in the column indicated by stratum id column. If there are no strata, then you can supply the same column used by _network-level-attributes.yml to indicate the unit code, and supply a single stratum area for the park.\nstratum id column: MDCATY stratum area info:  Gulley1: 684731  Gulley2: 138247  Upland: 1063552 The number assigned to each stratum under stratum area info corresponds to the total number of sites \u0026ndash; regardless of the actual units \u0026ndash; that could have been sampled in that stratum. In this example there were 684731 potentially sampleable sites in Gulley1. The proportion of the park represented by Gulley1 is 684731 / (684731 + 138247 + 1063552) = 0.3629579.1\nIn most cases, the sites actually sampled in any given stratum represents a tiny fraction of that stratum\u0026rsquo;s area (e.g., \u0026lt;1%). In cases where sampled sites represent a larger proportion of the total stratum area, we may need to apply finite population correction, which we\u0026rsquo;ll describe in another section of this guide.\n  Does the unit of the stratum areas matter so long as they are in the same unit? No, not yet, because these are only used to create the (normalized) stratum weights.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":1,"href":"/models-for-missing-data/docs/guide/outputs/00-intputs/","title":"Inputs","section":"Outputs","content":"Model inputs #  A collection of CSV and rds files\n"},{"id":2,"href":"/models-for-missing-data/docs/getting-started/installation/","title":"Installation","section":"Getting started","content":"Installation #  Obtaining a copy of the repository #  New here? #  To clone the repository, open a terminal1 and run the following command from the directory into which you\u0026rsquo;d like to place the project (e.g., from ~/repos).\ngit clone https://github.com/lzachmann/models-for-missing-data.git [DIRNAME] DIRNAME is optional, and specifies the name of the directory into which the project will be cloned on your local machine. You could call it \u0026ldquo;m4md\u0026rdquo; for instance, if you wanted something a bit\nExisting I\u0026amp;M uplands member? #  If you\u0026rsquo;re a member of the team that helped to create this tool, you may want to fetch the uplands data and analysis config files we\u0026rsquo;ve been using, which are kept in private submodules on GitLab.2\ngit clone --recurse-submodules --remote-submodules \\  https://github.com/lzachmann/models-for-missing-data.git [DIRNAME] The git clone command above will recurse through all of the project submodules and ask you to authenticate with GitLab as necessary.\nChecking for updates #  Project updates #  To get the latest models-for-missing-data code, navigate to your project directory using a terminal, and run git pull.\nSubmodule updates #  First, be sure to do any necessary housekeeping (remove, stash, or commit changes). To get the latest updates for each of the submodules, run:\ngit submodule update --recursive --remote If you encounter an error, it\u0026rsquo;s likely you\u0026rsquo;ve made changes locally that are not yet saved (staged and committed using git add and git commit). Git won\u0026rsquo;t replace changes in uncommitted files with changes on the remote by default. This is desirable behavior. Try committing your changes locally before syncing with the remote.\nPushing local changes to a submodule to its remote #  First, cd into the submodule directory. We\u0026rsquo;re going to do all of our Git work within the context of the submodule. If you\u0026rsquo;ve got uncommitted changes, do any necessary housekeeping:\ngit status git add . git commit -m \u0026#34;\u0026lt;some descriptive message about your changes\u0026gt;\u0026#34; As always, please ensure you\u0026rsquo;re not staging / committing unwanted files (e.g., binary files). Then run:\ngit fetch git checkout gh-submodule git merge \u0026lt;ref\u0026gt; git push origin gh-submodule   Windows users will need to use Git Bash to do this, which comes with your installation of Git for Windows\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If this command doesn\u0026rsquo;t work, try updating Git (--remote-submodules is only available in newer versions of Git). Alternatively, try removing --remote-submodules from the git clone command.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":3,"href":"/models-for-missing-data/docs/internals/model-builder/","title":"Model builder","section":"Internals","content":"Model builder #  The shell script compile-jags-file.sh builds JAGS model files using several arguments. The arguments \u0026ndash; which correspond to the likelihood, the deterministic function, group-level effects parameterization (random intercepts and slopes vs. random intercepts only), the presence or absence of additional covariates and, finally, the path to write to \u0026ndash; are supplied via the \u0026lsquo;MODEL\u0026rsquo; block in the YAML file for an analysis. For purposes of development or debugging, compile-jags-file.sh can be sourced from the command line with an optional output directory as such:\n./src/model-builder/compile-jags-file.sh \\ \u0026lt;likelihood\u0026gt; \u0026lt;deterministic-function\u0026gt; \u0026lt;group-level-effects\u0026gt; \u0026lt;covariates\u0026gt; \\ \u0026lt;write-path\u0026gt; For example:\n./src/model-builder/compile-jags-file.sh \\ poisson exponential b0-b1 w1 \\ assets The model compiler supports both random intercepts (b0) or random intercepts / random slopes (b0-b1) models with and without covariates.\n"},{"id":4,"href":"/models-for-missing-data/posts/non-ignorable-missingness/","title":"Non-ignorable missingness","section":"Posts","content":" Statistics is basically a missing data problem!\n \u0026ndash; Little 2013\nNearly all samples \u0026ndash; whether by design or by accident \u0026ndash; are incomplete. We very rarely make a complete census of all individuals in a population or all sites on a landscape. Sometimes we don\u0026rsquo;t collect, or can\u0026rsquo;t collect, complete information for individual samples or measures. For instance, we might know an animal was alive when it was last seen, so we know it survived at least that long, but know nothing about its current status. Or we might have information on the coverage of an invasive species down to a certain patch size, beyond which patches are too small or numerous to survey.\nAll of these examples involve missing data. It probably goes without saying that estimating population level parameters in light of such \u0026ldquo;missingness\u0026rdquo; is a challenge. We cannot compute a population mean if one or more values are missing.\n 6 NA 9 NA 9 NA 5 6 9 7 2  The mean for such a string of numbers is undefined.1 In some settings these missing data can be safely ignored, but these circumstances are rarely met in ecological studies. One helpful \u0026ldquo;trick\u0026rdquo; is to consider how the problem would be solved had the data been complete, and to consider what we might do to arrive at complete data. An incomplete data perspective provides a general framework for resolving this sampling problem and making inference.\n  If you\u0026rsquo;re an R person and you thought, \u0026ldquo;why not just include an na.rm = TRUE in the call to mean()\u0026rdquo;, kudos! You would indeed obtain a real number, but we don\u0026rsquo;t yet whether we can safely ignore these NA values.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":5,"href":"/models-for-missing-data/docs/guide/data/y-info/","title":"Response data","section":"Data","content":"Response data #  The response data include the observations we are trying to model, as well as columns with identifiers (indices, IDs, datetime strings) for all relevant design information. Elements of the sampling design often seen in long-term monitoring data include the following:\n   Design element Examples     observational units transect, quadrat, plot   sampling units plot, site   stratification stratum   date / event times MM/DD/YYYY, YYYY    Format #  The response information are stored as flat files, which are typically text files with no special word processing or markup. Each row represents a single observation, while the columns describe the value of the observation and the design features described above. The file can be CSV, XLS, XLSX, GZ, or RDS. For ease of use, readability, and other reasons, we generally recommend CSV.\nExample #  % select(-EventName, -OneEventPerYear, -native.rich, ) %% arrange(SiteName, Year) %% mutate(Row = row_number()) %% relocate(Row, Park) write_csv(bind_rows(head(d), tail(d)), 'docs/website/content/docs/guide/data/richness.csv') -- The response data below contain species richness observations for forb (native.forb.rich) and grass-like (native.gram.rich) species from Little Bighorn Battlefield National Monument (LIBI), in Montana. Here, we see the first and last six rows of the data.\n   Row Park MDCATY SiteName Year Transect Plot native.forb.rich native.gram.rich     1 LIBI Gulley1 LIBI_001 2011 1 A 6 5   2 LIBI Gulley1 LIBI_001 2011 1 B 5 5   3 LIBI Gulley1 LIBI_001 2011 1 C 4 2   4 LIBI Gulley1 LIBI_001 2011 1 D 6 8   5 LIBI Gulley1 LIBI_001 2011 2 A 6 7   6 LIBI Gulley1 LIBI_001 2011 2 B 4 6   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   1105 LIBI Upland LIBI_050 2019 2 A 5 2   1106 LIBI Upland LIBI_050 2019 2 B 4 6   1107 LIBI Upland LIBI_050 2019 2 C 3 3   1108 LIBI Upland LIBI_050 2019 3 A 2 1   1109 LIBI Upland LIBI_050 2019 3 B 2 1   1110 LIBI Upland LIBI_050 2019 3 C 3 2    Although they may go by different names, we see many of the design elements appearing in columns. Our sampling units are sites (SiteName) within strata (MDCATY). Individual observations are indexed by the unique combinations of Transect and Plot within each site. All of the sites in this dataset come from a single park unit (LIBI). The calendar year in which the observations were made is given in the column Year.\nFrom data to model #  We\u0026rsquo;ll see how to declare various aspects of the response information in the data block of the analysis config files in another section of this guide. For now, we will leave things as they stand, with two quick notes / caveats:\n Unlike many design based approaches, which aggregate observations within or across sampling units (using a mean, for instance), we work with the raw observations themselves. As we begin to develop models for the data, it will be important to know what type of data your observations represent. If you are expecting to use covariates in your model, the names of each design element must be consistent across response and covariate data files. The reason for this requirement is that the analysis pipeline performs auto joins. If the column containing site information in the response data is called SiteName, but Site in the covariate data, the program won\u0026rsquo;t know they\u0026rsquo;re intended to be the same. Additionally, even the entries within a column must be the same. Thus, if a site is called LIBI_001 in the response info it must be given the same name in the covariate info. If, in the covariate info, the sites appear without the prefix for unit code (e.g., 001), the join will fail.  "},{"id":6,"href":"/models-for-missing-data/docs/guide/config-files/data-block/response-info/","title":"Response info","section":"Data block","content":"Response information #  The usual case #  In the usual case, we are working with a single vector of observations (a single column of data in a flat file). Below, we see the first few lines of the data block for an analysis of count-type data from Little Bighorn Battlefield National Monument (LIBI), in Montana (a park within the Rocky Mountain Network). The file, rich.yml, lives in the directory assets/uplands-config/ROMN/LIBI.\n# ==== DATA ===================================================================  response info:  file: data/ROMN/LIBI_richness_20171208_r.csv  state variable:  response column:  - native.rich  - native.forb.rich  description:  - Native species richness  - Native forb species richness  sampling method: plot  sample id column(s):  - Transect  - Plot The response info section of the data block specifies the file (see the response data) for more detail on the contents of this file), the name of the column containing the response variable of interest (response column), a corresponding human-readable description for each response variable (description), and the names of the columns required to identify individual observations (sample id column(s)). In some cases, sample id column(s) is a single column, while in others it\u0026rsquo;s multiple.\n"},{"id":7,"href":"/models-for-missing-data/docs/guide/config-files/","title":"Config files","section":"Guide","content":"Config files #  We use YAML to create analysis configuration files. They are meant to be human-readable, and describe how a model relates to the data. We organize the high-level components into \u0026ldquo;blocks\u0026rdquo;. The blocks are not in-and-of-themselves useful, but provide sensible organization to a specification of an analysis. They include:\n The data (response, site location, and covariate information) The model (likelihood, link function(s), group-level effects, predictor variables, etc.) Other quantities (posterior predictive checks, etc.)  For more background on YAML files, see this helpful tutorial.\n"},{"id":8,"href":"/models-for-missing-data/docs/guide/data/x-info/","title":"Covariate data","section":"Data","content":"Covariate data #  Covariates are variables that are expected to change with the response variable \u0026ndash; they covary with in some way with the observations we seek to model. The definition of a covariates varies widely online and in the literature. For our purposes, we use the term covariate to describe any variable (whether continuous or discrete) that might influence the mean of the response variable we are interested in. In many cases their effects are of direct interest in the analysis (weather or terrain, for instance). In other cases, a covariate might be \u0026ldquo;nuisance variable\u0026rdquo; \u0026ndash; a fact or feature that is of no particular interest in itself, but nonetheless might be necessary to build a proper model and develop robust inference. Examples of nuisances include sudden changes in protocol or observers.\nCovariates can be static or dynamic. A variable like the elevation of site varies from site to site, but not in time (at least on the time scales we are interested in). Variables involving weather, such as precipitation, are more interesting. A long-run summary (e.g., 30-year average rainfall), like our static terrain variable, might only vary in space. However, an annualized metric will vary in time and space.\nThe granularity of information can also vary. Sometimes the information we have is rather coarse, so many sites have the same value. Gridded climate data are an excellent example of relatively coarse (e.g., 1km gridcell) data. Adjacent sites falling into the same gridcell will have the same value.\nFormat #  As with the response data, covariates are typically stored as flat files.\nExample #  % select(MDCATY, SiteName, Year, Botanist, deficit.pregr) %% arrange(SiteName, Year) %% mutate(Row = row_number()) %% relocate(Row) write_csv(bind_rows(head(d), tail(d)), 'docs/website/content/docs/guide/data/richness-covariates.csv') -- The data below contain site (SiteName), stratum (MDCATY), time (Year), and covariate information (Botanist and deficit.pregr) for sites from Little Bighorn Battlefield National Monument (LIBI), in Montana. Here, we see the first and last six rows of the data.\n   Row MDCATY SiteName Year Botanist deficit.pregr     1 Gulley1 Grid_100 2009 DS 418.7766661   2 Gulley1 Grid_100 2010 DS 434.9612421   3 Gulley1 Grid_100 2011 JA 408.9919742   4 Gulley1 Grid_100 2012 JA 450.2639661   5 Gulley1 Grid_100 2013 JA 532.0683071   6 Gulley1 Grid_100 2014 JA 437.0008004   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   3240 Upland LIBI_050 2014 JA 381.9736062   3241 Upland LIBI_050 2015 JA 328.6534898   3242 Upland LIBI_050 2016 JA 431.6948948   3243 Upland LIBI_050 2017 JA 402.8102409   3244 Upland LIBI_050 2018 JA 393.2275301   3245 Upland LIBI_050 2019 JA 247.7891026    It\u0026rsquo;s worth pointing out a few things. First, we see two different types of variables \u0026mdash; a categorical variable containing the initials of the observing botanist, and a continuous variable for pre growing season water deficit. Second, the granularity of the covariates is limited to the site, but both variables appear to vary in time. Finally, we see more rows that we might expect based on the example data seen in the response data section. The reason for this is two-fold:\n The covariate data at a site include values for all years over the duration of the study, whether that site was sampled or not. In general, sites are visited on a rotating basis, meaning they\u0026rsquo;re not sampled every year. There are sites (e.g., Grid_100) that were never sampled. We include this information because it\u0026rsquo;s needed to make inference at the park scale, and because we\u0026rsquo;re interested in making predictions of our focal response variables at every site on the landscape, whether it was visited by field crews or not.  From data to model #  We\u0026rsquo;ll see how to include covariates in models using the XX and YY blocks of the analysis config files in other sections of this guide.\n"},{"id":9,"href":"/models-for-missing-data/docs/guide/outputs/01-diagnostics/","title":"Diagnostics","section":"Outputs","content":"Model diagnostics #  Gelman and rubin statistics, traceplots, etc.\n"},{"id":10,"href":"/models-for-missing-data/posts/trend-vs-trajectory/","title":"Disentangling concepts of status, trend, and trajectory","section":"Posts","content":"The terms status and trend are ubiquitous in resource monitoring and management settings. To be useful and robust, however, they require precise (mathematical) definitions. It has been my experience that misunderstanding these terms can lead to misapplication of model predictions and to researchers and managers drawing the wrong conclusions from the data. In this post we show how relatively simple, even intuitive, definitions for each of these terms clarifies their intent, and improves the insights provided by models of monitoring data.\nFor context, it is helpful to see how these terms have been used previously in the literature\n The ‘term “trend” [is] a description of an overall tendency, without regard to fluctuations in the trajectory. The distinction between the terms has important consequences for analysis and interpretation of survey data. Trend describes the change in a population over a specific interval; trajectory describes the manner in which the change occurred.\n \u0026ndash; Link and Sauer 1998\nTrend vs. trajectory (Citation: Link\u0026#32;\u0026amp;\u0026#32;Sauer,\u0026#32;1998Link,\u0026#32; W.\u0026#32;\u0026amp;\u0026#32;Sauer,\u0026#32; J. \u0026#32; (1998). \u0026#32;Estimating population change from count data: Application to the north american breeding bird survey. Ecological applications,\u0026#32;8(2).\u0026#32;258–268. ) .\n References #    Link\u0026#32;\u0026amp;\u0026#32;Sauer (1998)  Link,\u0026#32; W.\u0026#32;\u0026amp;\u0026#32;Sauer,\u0026#32; J. \u0026#32; (1998). \u0026#32;Estimating population change from count data: Application to the north american breeding bird survey. Ecological applications,\u0026#32;8(2).\u0026#32;258–268.     "},{"id":11,"href":"/models-for-missing-data/docs/guide/config-files/model-block/likelihood/","title":"Likelihood","section":"Model block","content":"Likelihood #  "},{"id":12,"href":"/models-for-missing-data/docs/guide/config-files/model-block/link/","title":"Link function","section":"Model block","content":"Deterministic (link) function for the regression on the mean #   deterministic model:  - inverse-logit "},{"id":13,"href":"/models-for-missing-data/docs/guide/config-files/data-block/site-loc-info/","title":"Location info","section":"Data block","content":"Location info #  Site location information (site location info), if provided, is used to evaluate spatial autocorrelation in model residuals.\nsite location info:  file: assets/uplands-data/ROMN/LIBI_GRKO_metrics_adjwt_20170414_forCSP.csv  coordinate columns:  - xcoord  - ycoord Here again, we point to the name of the file (file) and supply the names of the columns containing the spatial coordinates (coordinate columns). Note that proper residual analysis requires Cartesian coordinates. If the locations come directly from a geographic coordinate system, they\u0026rsquo;ll need to be transformed to a projected coordinate system.\n"},{"id":14,"href":"/models-for-missing-data/docs/getting-started/requirements/","title":"Requirements","section":"Getting started","content":"Requirements #  The requirements include R (v4.X.X) and RStudio (if you\u0026rsquo;d like to work with models-for-missing-data interactively), and JAGS (v4.3.0). Git and Git Bash (for Windows users) are convenient if you\u0026rsquo;d like to stay up to date with the latest changes.\nThe requirements (including a few non-essentials) are described in the latest Dockerfile. You\u0026rsquo;re likely to need everything you see beneath install2.r in the instructions below. You can install these from your R console with install.packages(). FROM rocker/geospatial:4.1.2  # Copy Latin Modern font files to fonts directory and refresh fonts cache. COPY Latin-Modern-Roman-fontfacekit.zip /tmp RUN unzip tmp/Latin-Modern-Roman-fontfacekit.zip -d /usr/share/fonts \u0026amp;\u0026amp; \\  fc-cache -f -v \u0026amp;\u0026amp; \\  rm tmp/Latin-Modern-Roman-fontfacekit.zip  # Install JAGS. RUN apt-get update -y \u0026amp;\u0026amp; apt-get install -y jags  # Graphics and other required packages. RUN apt-get install tree \u0026amp;\u0026amp; \\  Rscript -e \u0026#34;update.packages(ask = FALSE)\u0026#34; \u0026amp;\u0026amp; \\  install2.r --error \\  abind \\  spsurvey \\  hrbrthemes \\  ggthemes \\  ggridges \\  cowplot \\  HDInterval \\  rjags \\  coda \\  R2jags \\  runjags \\  bayesplot \\  extraDistr \\  MCMCpack \\  magick \\  gifski \\  gganimate \\  multidplyr \n"},{"id":15,"href":"/models-for-missing-data/docs/guide/config-files/data-block/covariate-info/","title":"Covariate info","section":"Data block","content":"Covariate info #  Covariate information (covariate info), if provided, is joined to the response information prior to model fitting. It\u0026rsquo;s used only in models for which predictors are specified.\ncovariate info:  file: assets/uplands-data/ROMN/LIBI_Covariates_AllSitesAllYears_20201104_Through2016_with_exotics.csv  event date info:  date-time column: Year  date-time format: Y! As with the other statements in the data block, we point to the name of the covariate file (file; see the covariate data section for more detail) and, if different from the date attributes in the response data, supply info required to properly parse covariate datetime fields.\n"},{"id":16,"href":"/models-for-missing-data/docs/guide/config-files/model-block/group-level-effects/","title":"Group-level effects","section":"Model block","content":"Group-level (random) effects #   # Group-level effects structure. One of: b0, b0-b1.  group-level effects:  - b0  - b0-b1 "},{"id":17,"href":"/models-for-missing-data/docs/guide/outputs/","title":"Outputs","section":"Guide","content":"Outputs #  All outputs (including logs) are stored in the \u0026lsquo;output\u0026rsquo; directory, which mirrors \u0026ndash; almost exactly \u0026ndash; the structure of the \u0026lsquo;config\u0026rsquo; directory. The only difference is that the results of analyses for each state variable are stored separately, whereas the configuration of analyses for like-variables (related count or cover variables, for example) can be specified simultaneously in the same YAML file.\n"},{"id":18,"href":"/models-for-missing-data/docs/getting-started/quickstart/","title":"Quickstart","section":"Getting started","content":"Quickstart #  Assuming you\u0026rsquo;ve got the models-for-missing-data project code (see Installation) and have all of the dependencies (see Requirements), the last remaining pieces you\u0026rsquo;ll need to begin running models are the data and analysis config files. We\u0026rsquo;ve included examples of each in the \u0026lsquo;assets/\u0026rsquo; directory. See the guide for more information about each of these components of the models-for-missing-data workflow.\nComponents #  Data #  Config files #  Usage #  There are two basic ways to call models-for-missing-data for analyses: interactively using RStudio or non-interactively via the command line.\nGraphical user interface (RStudio) #  Start RStudio and set your working directory to the project directory. In \u0026lsquo;model-api/\u0026rsquo; create a copy of \u0026lsquo;analysis-pipeline.R\u0026rsquo;, perhaps by appending \u0026rsquo;local\u0026rsquo; or your initials as a suffix. E.g., \u0026lsquo;analysis-pipeline-local.R\u0026rsquo;.\nCommand line interface (CLI) #  Run an analysis utilizing 2 CPUs with, for example:\n./model-api/analysis-pipeline.R \\ assets/_config/M4MD/ELDO/counts.yml \\ --n-adapt 5000 --n-update 50000 --n-iter 15000 \\ --n-cores 2 See ./model-api/analysis-pipeline.R --help for more details on all CLI arguments, options, and flags.\n"},{"id":19,"href":"/models-for-missing-data/docs/guide/data/types-of-random-variables/","title":"Types of random variables","section":"Data","content":"Types of random variables #  Understanding the type of data you are working with will be extremely important as we begin to define models.\nContinuous random variables #    Discrete random variables #    "},{"id":20,"href":"/models-for-missing-data/posts/unequal-inclusion-probability/","title":"Unequal inclusion probabilities","section":"Posts","content":"The Sonoran Desert is among the most extreme environments on Earth. Sampling in these remote, rugged landscapes requires a different approach. When the Park Service established monitoring in Organ Pipe Cactus National Monument they used an approach to select sites based on the cost of travel to sites on the broader landscape, visiting less \u0026ldquo;costly\u0026rdquo; sites with higher probability than more costly sites. The cost surface that defined the probability of inclusion of sites was developed using terrain data, and a tool that estimates the time to travel to any arbitrary location on the landscape.\n"},{"id":21,"href":"/models-for-missing-data/posts/sampling-and-populations/","title":"Sampling and populations","section":"Posts","content":"We sample for a very practical reason. It\u0026rsquo;s usually impossible to get information on the whole population, so we use a sample to make inferences about the population. In our case, the population is typically all sites in a stratum or all sites \u0026ndash; in all strata \u0026ndash; at the scale of an entire park. Typically, the inference we seek entails three questions.\n What\u0026rsquo;s the best estimate of the population mean?   We can generate a sample mean,   \\(\\bar{x}\\)  , from our sample. This is the best estimate of the population mean.\n  \\[\\bar{x}=\\frac{\\sum{x}}{n}\\]  How confident are we about that estimate?   Because we have a small sample, we can\u0026rsquo;t be sure the sample mean is exactly the population mean. There\u0026rsquo;s uncertainty around that estimate. The standard error of the sample mean,  \\(\\text{SE}(\\bar{x})\\)  , relates to how uncertain we are about our estimate. The higher it is, the more uncertain we are that our sample mean reflects the true population mean. Small samples generally yield larger standard errors.\n  \\[\\text{SE}(\\bar{x})=\\frac{s}{\\sqrt{n}}\\]  What is our best estimate of the population standard deviation?1   The sample standard deviation,  \\(s\\)  , can give us information about the possible population standard deviation \u0026ndash; how spread the population is likely to be with respect to the measure of interest.\n  \\[s=\\sqrt{\\frac{\\sum{(x - \\bar{x})^2}}{n-1}}\\]  These three measures form the basic substrate of our understanding of the populations we are sampling, and are all essential to decision making.\nBelow, we consider three scenarios, sampling from an infinite population, a finite population, and the full population. The goal of the examples below is to show what sampling is all about. As we shall see, the finite population case is the bridge between the other two edge cases \u0026ndash; the infinite and full population instances.\nAn infinite population #  This is a very typical scenario. Of course, I\u0026rsquo;m not aware of any truly infinite populations. What we mean, instead, is that our sample of the population,  \\(n\\)  , is small enough that\u0026hellip; blah blah blah. As such, we can think of the population as at least theoretically infinite.\nExample Estimate the average number of trees in sites #  Given a sample of  (n=5)  sites with observations\u0026hellip;, the sample mean is XX. This our best estimate of the true population mean. But what is the population we are referring to? Well, it\u0026rsquo;s all possible sites. Of course, there\u0026rsquo;s not an infinite number of sites, but it\u0026rsquo;s large enough that we can treat it as though it is infinite. How confident are we in this estimate? If we took a different sample, the sample mean would be different, but how different? This is where the standard error of the sample mean comes in\u0026hellip;   A finite population2 #  Not particularly common, but it\u0026rsquo;s useful to look at because it ties together both the infinite population situation and the final example.\n \\[\\text{SE}(\\bar{x})=\\frac{s}{\\sqrt{n}}\\sqrt{\\frac{N - n}{N - 1}}\\]  The standard error of the mean in this case will be less than in the infinite population case, because we have proportionately more information about the population that we do in the first example.\nThe full population #  Technically no longer a sample. We have the full population in hand. There\u0026rsquo;s no observations that site outside of the sample that we care about.\n \\[\\mu=\\frac{\\sum{x}}{N}\\]  How confident are we in this estimate? Perfectly confident:  \\(\\text{SE}(\\mu)=0\\)  ! Why? We can still think of this as a sample from a finite population, but the number of observations in the sample equals the number of observations in the population. The formula for the standard error of the mean for a finite population still applies. The numerator in the second term  \\(N-n\\)  becomes zero, and the whole thing collapses to zero.3\nWe can find the population standard deviation.\n \\[\\sigma=\\sqrt{\\frac{\\sum{(x - \\mu)^2}}{N}}\\]    Note that part of the  \\(\\text{SE}(\\bar{x})\\)  is in fact  \\(s\\)  .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Sometimes called \u0026ldquo;sampling without replacement\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Note that the same basic reasoning can be applied to the first example, as well. If we were to look at an infinite population and sub in Inf for the capital Ns. The second term resolves to 1. Inf on inf is going to be 1, the sqrt of which is also 1, so the second term dissapears and you\u0026rsquo;re left with the original formula for the standard error.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":22,"href":"/models-for-missing-data/posts/interpreting-coefficients/","title":"Interpreting coefficients","section":"Posts","content":"Making sense of the effects of variables included as predictors #  Some aspects of covariate effects are readily apparent \u0026ndash; for instance, the sign of a coefficient in a model says at least something about the general directionality of the effect, positive or negative. However, a deeper understanding of a model typically requires inferences that go well beyond simple measures of the directionality or significance of effects \u0026ndash; it requires understanding the size of effects.\nWhat do we mean by effect size, and how is it measured? In order to offer a working definition of effect size, we need to use a bit of math. The size of an effect can be approximated as the average change in the response variable being modeled, per unit change in some predictor variable, which we\u0026rsquo;ll call   \\(x_1\\)   \\[\\mu_i = g(\\beta_0 \u0026#43; \\beta_1 x_{1i}).\\]  The parameter  \\(\\beta_0\\)  is the intercept and  \\(\\beta_1\\)  is the slope associated with covariate  \\(x_1\\)  in a generalized linear model (linear, exponential, or inverse logit), which we represent using the link function  \\(g()\\)  . The effect of  \\(x_1\\)  translates into changes in the mean of the response variable through  \\(\\beta_1\\)  and link function  \\(g()\\)  . To understand the effect that  \\(x_1\\)  will have on the mean on the scale of the actual data, we first have to take note of the function for which  \\(g()\\)  is a placeholder, as well as any transformations (e.g., scaling) that may have been applied to the data.\nExample | the response of plant cover to rainfall #  Let\u0026rsquo;s consider a covariate that we might expect to have a mechanistic influence on plant cover, such as accumulated spring precipitation. Thus,  \\(x_1\\)  is a measure of rainfall. The plant cover observations were collected at point intercepts along a transects at a site, indexed by subscript  \\(i\\)  . At any given point intercept, plants are either present or absent.  \\(y_i\\)  , then, is the sum of the number of times plants were detected (the total number of \u0026ldquo;hits\u0026rdquo;) at any point intercept along a transect. The observations are modeled as\n \\[\\begin{align*} p_i \u0026amp;= \\text{logit}^{-1}(\\beta_0 \u0026#43; \\beta_1 x_{1i}) \\\\ y_i \u0026amp;\\sim \\text{binomial}(n, p_i) \\end{align*}\\]  where  \\(n\\)  is the total number of point intercepts evaluated along each transect (the number of \u0026ldquo;trials\u0026rdquo;).  \\(x_1\\)  is scaled from its original units (cm) to zero mean and unit variance prior to fitting the model.\nLet\u0026rsquo;s also take as a given that the estimated median value of the coefficients is as follows:  \\(\\beta_0=0.12\\)  and  \\(\\beta_1=0.22\\)  . Prior to scaling,  \\(x_1\\)  had mean 20.4, standard deviation 15.3, and range [0.4, 70.9]. To better understand the influence of  \\(x_1\\)  on the mean plant cover, we will create a new vector called  \\(x_1^{\\text{pred}}\\)  , which consists of scaled rainfall values at equally spaced intervals over the range of  \\(x_1\\)  . Here is the basic setup, in  \\(\\textsf{R}\\)  :\nb0 \u0026lt;- 0.12 # the intercept b1 \u0026lt;- 0.22 # the effect of spring precipitation  x1_mean \u0026lt;- 20.4; x1_sd \u0026lt;- 15.3; x1_range \u0026lt;- c(0.4, 70.9) x1_pred_raw \u0026lt;- seq(x1_range[1], x1_range[2], length.out = 100) x1_pred \u0026lt;- (x1_pred_raw - x1_mean) / x1_sd Let\u0026rsquo;s plot mean plant cover, p, over the range of x1_pred:\np \u0026lt;- boot::inv.logit(b0 + b1 * x1_pred) p_x1_mean \u0026lt;- boot::inv.logit(b0) # p at average precip plot(x1_pred_raw, p, ylim = c(0, 1), type = \u0026#39;l\u0026#39;, col = \u0026#39;red\u0026#39;) abline(v = x1_mean, lty = 2, col = \u0026#39;gray\u0026#39;) abline(h = boot::inv.logit(b0), lty = 2, col = \u0026#39;gray\u0026#39;) Mean plant cover increases as a function of spring rainfall, from a value of approximately 0.46 at its minimum (next to zero precipitation) to 0.70 at its maximum (~71 cm of rainfall). We can interpret the quantity  \\(\\text{logit}^{-1}(\\beta_0)=\\text{logit}^{-1}(0.12)=0.53\\)  (the horizontal dashed gray line) as the mean site-level cover of plants in a year with average spring rainfall (vertical dashed gray line).1\n This being an inverse logit model, we are somewhat obligated to talk about odds, which are given by the quantity  \\(\\frac{p}{(1-p)}\\)  . In this example, it\u0026rsquo;s the ratio of the probability of a plant being present at a given point intercept to the probability of a point intercept not touching a plant. In a year with average spring rainfall, you are ~1.13X more likely to see it than not (0.53 / (1 - 0.53)).2\n \\(\\beta_1\\)  is the multiplicative change in odds per standard deviation spring precipitation.3 To compute the odds of seeing plant cover if spring rainfall is one standard deviation higher than average we would use\n(odds_x1_mean \u0026lt;- p_x1_mean / (1 - p_x1_mean)) exp(b1) * odds_x1_mean # 1.4 At 1.4X, the odds of encountering plants has gone up. Basically, when rainfall is one standard deviation over the mean, you\u0026rsquo;re now 1.4X more likely to see it than not. Thus, the odds of seeing plants increases with increasing precipitation. We can show the same thing using slightly different math.\np_x1_sd1 \u0026lt;- boot::inv.logit(b0 + b1 * 1) # let\u0026#39;s make sure we can recover the same odds using different math p_x1_sd1 / (1 - p_x1_sd1) # again, 1.4! Let\u0026rsquo;s show the odds at progressively more extreme values of rainfall ( \\(1\\sigma, 2\\sigma, 3\\sigma\\)  ):\nplot(x1_pred_raw, p, ylim = c(0, 1), type = \u0026#39;l\u0026#39;, col = \u0026#39;red\u0026#39;) abline(v = x1_mean, lty = 2, col = \u0026#39;gray\u0026#39;) abline(h = p_x1_mean, lty = 2, col = \u0026#39;gray\u0026#39;) # at 1 sd lines(x = rep(x1_mean + x1_sd, 2), y = c(0, boot::inv.logit(b0 + b1 * 1)),  lty = 2, col = \u0026#39;orange\u0026#39;) points(x = x1_mean + x1_sd, y = boot::inv.logit(b0 + b1 * 1)) # at 2 sd lines(x = rep(x1_mean + 2*x1_sd, 2), y = c(0, boot::inv.logit(b0 + b1 * 2)),  lty = 2, col = \u0026#39;green\u0026#39;) points(x = x1_mean + 2*x1_sd, y = boot::inv.logit(b0 + b1 * 2)) # at 3 sd lines(x = rep(x1_mean + 3*x1_sd, 2), y = c(0, boot::inv.logit(b0 + b1 * 3)),  lty = 2, col = \u0026#39;blue\u0026#39;) points(x = x1_mean + 3*x1_sd, y = boot::inv.logit(b0 + b1 * 3)) When rainfall is near it\u0026rsquo;s observed maximum value (at 3 standard deviations higher than the mean), you are now exp(b1)^3 or ~2.2X more likely to see plants than not.\n Effect size inference using another scale #  Let\u0026rsquo;s say the manager isn\u0026rsquo;t thrilled about using standard deviations in rainfall as the unit of measure. Instead, they\u0026rsquo;re focused on inches of precipitation. How do we cast these effect size calculations in terms of inches and not standard deviations? First we need to recover the unstandardized slope term, which we\u0026rsquo;ll notate as  \\(\\beta_1^{\\prime}\\)  using  \\(\\beta_1^{\\prime}=\\frac{\\beta_{1}}{\\text{sd}(\\mathbf{x}_{1})}\\)  .\nb1_unscaled \u0026lt;- b1 / x1_sd exp(b1_unscaled) # per cm exp(b1_unscaled * 2.54) # per inch exp(b1_unscaled) is the multiplicative change in odds per cm spring rainfall. It\u0026rsquo;s much smaller than exp(b1) because x1_sd is ~15cm of precip!). We now want inches. So it\u0026rsquo;s b1_unscaled * 2.54.\n  After scaling, the value of  \\(x_1\\)  in an average rainfall year is simply zero. The term involving rainfall in the model drops out because  \\(\\beta_1 \\times 0 = 0\\)  and  \\(\\text{logit}^{-1}(0.12 \u0026#43; 0)\\)  simplifies to  \\(\\text{logit}^{-1}(0.12)\\)  .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The odds here make at least some intuitive sense. At p = 0.53 you\u0026rsquo;re just slightly more likely than not to encounter plants at a given point at this site.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Why is this the case? Inverse logit models have the form  \\[\\begin{align*} p_{i} \u0026amp;= \\frac{e^{\\beta_{0}\u0026#43;\\beta_1x_{1i}}}{1\u0026#43;e^{\\beta_{0}\u0026#43;\\beta_1x_{1i}}} \\\\ \u0026amp;= \\text{logit}^{-1}(\\beta_0 \u0026#43; \\beta_1 x_{1i}) \\end{align*}\\]  Rearranging the equation above, we obtain  \\[\\text{logit}(p_{i})=\\text{log}\\left(\\frac{p_{i}}{1-p_{i}}\\right)=\\beta_{0}\u0026#43;\\beta_1x_{1i}\\]   \\(\\beta_1\\)  can be understood somewhat more intuitively by exponentiating both sides of the equation above  \\[\\begin{align*} \\frac{p_{i}}{1-p_{i}} \u0026amp;= e^{\\beta_{0}\u0026#43;\\beta_1x_{1i}}\\\\ \u0026amp;= e^{\\beta_{0}} \\times e^{\\beta_1x_{1i}} \\end{align*}\\]  \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   "},{"id":23,"href":"/models-for-missing-data/posts/stratum-varying-fixed-effects/","title":"Stratum-varying fixed effects","section":"Posts","content":"Assume we have three strata,   \\(s_0\\)  ,  \\(s_1\\)  , and  \\(s_2\\)  , where  \\(s_0\\)  is the \u0026ldquo;reference\u0026rdquo; stratum – in other words,  \\(s_0\\)  is the stratum for which the 0/1 indicator is 0 across the board in the indicator matrix below (the first row):\n \\[\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 1 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}\\]  B_0 + (B_1 + B_1_s1_offset * s1 + B_1_s2_offset * s2) * x_1  # in stratum s0 B_0 + (B_1) * x_1  # in stratum s1 B_0 + (B_1 + B_1_s1_offset * s1) * x_1  # in stratum s2 B_0 + (B_1 + B_1_s2_offset * s2) * x_1  # lm(y~x1*x2) model.matrix(~x1*x2, tibble(x1 = runif(5), x2 = runif(5))) "},{"id":24,"href":"/models-for-missing-data/posts/offsets/","title":"The offset term","section":"Posts","content":"Counts of things naturally scale with the length or duration of observation, the area sampled, and sampling intensity (Citation: McElreath,\u0026#32;2018McElreath,\u0026#32; R.\u0026#32; (2018). \u0026#32;Statistical rethinking: A bayesian course with examples in r and stan. \u0026#32; Chapman; Hall/CRC. ) . For instance, the longer the river stretch we survey, the more fish we\u0026rsquo;ll tend to find.\nOffset terms are used to model rates \u0026ndash; e.g., counts per unit area or time. In the context of the model, the offset term transforms the response variable from a rate to a count.\nWhy do we need an offset? #  Why can\u0026rsquo;t we simply derive a new response variable by normalizing the original counts by the relevant sampling unit? The distributions we use to model counts have support for discrete-valued variables. Transformations of the counts can violate that requirement. For instance, say we counts signs of human disturbance in 20m-by-50m plots. If we see 60 signs, that\u0026rsquo;s 60 / (20 * 50) = 0.06. Although the mean of a Poisson distributed variable can be continuous, observations cannot be. Thus, transforming the response variable from counts to a rate is incorrect.\nThe offset needs to be on the same scale as the linear predictor. In the case of a log link model, this requires the offset variable to be logged before inclusion in the model  poi_r \u0026lt;- glm(numclaims ~ x1+x2+x3,data=train, family = \u0026ldquo;poisson\u0026rdquo;, offset=log(exposure))\nTypically we use offsets when the sampling unit (time, area, exposure) varies across observations. For example, species richness in riparian areas in arid ecosystems is often evaluated for the entire riparian area, which tend to be quite small. But the actual extent of each riparian area might differ.\n References #    McElreath (2018)  McElreath,\u0026#32; R.\u0026#32; (2018). \u0026#32;Statistical rethinking: A bayesian course with examples in r and stan. \u0026#32; Chapman; Hall/CRC.     "},{"id":25,"href":"/models-for-missing-data/docs/best-practices/","title":"Best practices","section":"Docs","content":"Best practices #  Ideally, you\u0026rsquo;ve got some expertise or experience working with the data you\u0026rsquo;re interested in modeling. This means you know the system, and might have some sense for what the likely drivers of change or trend might be.\n"},{"id":26,"href":"/models-for-missing-data/docs/faq/","title":"FAQ","section":"Docs","content":"Frequently asked questions #  Q: Which type of data am I working with?\nA: Let\u0026rsquo;s talk about it!\nQ: Do we use stratum weights when we do the finite population correction? A: Yes.\nQ: Should the locations I supply be for sites or samples within sites? A: For now, just the site centroids.\nQ: Why do my variography plots show only a fraction of the largest distance between sites?\nA: The R functions we are using to develop these plots only show semivariance for distances up to 1/3 of the maximum observed distance.\n"}]